#version 450
#extension GL_KHR_shader_subgroup_basic: enable
#extension GL_KHR_shader_subgroup_ballot: enable
#extension GL_KHR_shader_subgroup_arithmetic: enable
#extension GL_KHR_shader_subgroup_vote: enable
#extension GL_KHR_shader_subgroup_shuffle_relative : enable
#extension GL_KHR_shader_subgroup_shuffle : enable

#define PART_SIZE       7680    //The partition tile size of a  threadblock
#define VEC_SIZE      1920    //The total size of all subgroup histograms in shared memory
#define WORKGROUP_SIZE gl_WorkGroupSize.x;

#define VECTOR_MASK         3       //Mask of uint4 size aka 4 - 1
#define VECTOR_LOG          2       //log2(4) 

#define FLAG_NOT_READY  0          
#define FLAG_AGGREGATE  1          
#define FLAG_INCLUSIVE  2           
#define FLAG_MASK       3           


layout
void ChianedDecoupledInclusive(){

    // clear shared memory
    for(uint i = gl_LocalInvocationID.x; i < BIN_HISTS_SIZE; i += gl_WorkGroupSize.x){
        s_subgroupHistograms[i] = 0;
    }

    // assign partition tiles
    if (gl_LocalInvocationID.x == 0)
        s_subgroupHistograms[BIN_PART_SIZE - 1] = atomicAdd(b_index[radix_shift >> 3], 1);
    groupMemoryBarrier();
    barrier();

    const uint partitionIndex = s_subgroupHistograms[BIN_PART_SIZE - 1];

    // load global histogram into shared memory
    if (gl_LocalInvocationID.x < RADIX_BIN)
        s_localHistograms[gl_LocalInvocationID.x] = b_globalHist[gl_LocalInvocationID.x + (radix_shift << 5)];
    
    
    // handle size that is not prect multiple of parition size
    if (partitionIndex < gl_NumWorkGroups.x -1){
        // load the keys
        uint keys[BIN_KEYS_PER_THREAD];
        for (uint i = 0, t = gl_SubgroupInvocationID + BIN_SUB_PART_START + BIN_PART_START; i < BIN_KEYS_PER_THREAD; i++, t += LANE_COUNT){
            keys[i] = b_sort[t];
        }

        uint _offsets[BIN_KEYS_PER_THREAD];
        
        // warp level mutli-split
        for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i){
            uint warpFlags = 0xFFFFFFFF;
            for (int k = 0; k < RADIX_LOG; ++k){
                const bool t2 = (keys[i] >> k + radix_shift & 1) > 0;
                warpFlags &= (t2 ? 0 : 0xFFFFFFFF) ^ subgroupBallot(t2).x;
            }

            // Number of peers having same digit as key[i]
            // maybe it is wrong?
            const uint bits = subgroupBallotExclusiveBitCount(uvec4(warpFlags,0,0,0));//bitCount(warpFlags & getLaneMaskLt());

            uint prev;
            if (bits == 0)
                prev = atomicAdd(s_subgroupHistograms[(gl_SubgroupID.x << RADIX_LOG) + ((keys[i] >> radix_shift) & RADIX_MASK)], bitCount(warpFlags));
            int shuffleIndex = int(findLSB(warpFlags));
            _offsets[i] = subgroupShuffle(prev, shuffleIndex) + bits;
        }
        
        groupMemoryBarrier();
        barrier();

        // exclusive prefix scan up the  warp histograms
        if (gl_LocalInvocationID.x < RADIX_BIN){
            uint reduction = s_subgroupHistograms[gl_LocalInvocationID.x];
            for (uint i = gl_LocalInvocationID.x +  RADIX_BIN; i < BIN_HISTS_SIZE; i += RADIX_BIN){
                reduction += s_subgroupHistograms[i];
                s_subgroupHistograms[i] = reduction - s_subgroupHistograms[i];
            }
            //uint val = ((partitionIndex > 0) ? FLAG_AGGREGATE : FLAG_INCLUSIVE) | reduction << 2;
            atomicAdd(b_passHist[gl_LocalInvocationID.x* gl_NumWorkGroups.x + partitionIndex]/*[pass_num]*/,((partitionIndex > 0) ? FLAG_AGGREGATE : FLAG_INCLUSIVE) | (reduction << 2));
            // exclusive prefix sum across reduction
            s_subgroupHistograms[gl_LocalInvocationID.x] = InclusiveWarpScanCircularShift(reduction);
        }
        groupMemoryBarrier();
        barrier();

        if (gl_LocalInvocationID.x < (RADIX_BIN >> LANE_LOG))
            s_subgroupHistograms[gl_LocalInvocationID.x << LANE_LOG] = subgroupExclusiveAdd(s_subgroupHistograms[gl_LocalInvocationID.x << LANE_LOG]);//ActiveExclusiveWarpScan(s_subgroupHistograms[gl_LocalInvocationID.x << LANE_LOG]);
        groupMemoryBarrier();
        barrier();

        if ((gl_LocalInvocationID.x  < RADIX_BIN) && (gl_SubgroupInvocationID) > 0)
            s_subgroupHistograms[gl_LocalInvocationID.x] += subgroupBroadcast(s_subgroupHistograms[gl_LocalInvocationID.x -1], 1); // or subgroupShuffle(s_subgroupHistograms[gl_LocalInvocationID.x], 1)
        groupMemoryBarrier();
        barrier();

        // update offsets
        if (gl_SubgroupID > 0){
            for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i){
                const uint t2 = keys[i] >> radix_shift & RADIX_MASK;
                _offsets[i] += s_subgroupHistograms[(gl_SubgroupID << RADIX_LOG) + t2] + s_subgroupHistograms[t2]; 
            }
        }else{
            for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i)  {
                _offsets[i] += s_subgroupHistograms[keys[i] >> radix_shift & RADIX_MASK];
            }
        }

        // split the warps into single thread cooperative groups and lookback
        if (gl_LocalInvocationID.x < RADIX_BIN){
            if (partitionIndex > 0){
                uint reduction = 0;
                for( int k = int(partitionIndex); k > 0; ){
                    const uint flagPayload = b_passHist[gl_LocalInvocationID.x * gl_NumWorkGroups.x + k - 1];//[pass_num];
                    
                    if ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE){
                        reduction += flagPayload >> 2;
                        atomicAdd(b_passHist[gl_LocalInvocationID.x * gl_NumWorkGroups.x + partitionIndex]/*[pass_num]*/, 1 | (reduction << 2));
                        s_localHistograms[gl_LocalInvocationID.x] += reduction - s_subgroupHistograms[gl_LocalInvocationID.x];
                        break;
                    }

                    if ((flagPayload & FLAG_MASK) == FLAG_AGGREGATE){
                        reduction += flagPayload >> 2;
                        k--;
                    }
                }
                
            }else{
                s_localHistograms[gl_LocalInvocationID.x] -= s_subgroupHistograms[gl_LocalInvocationID.x];
            }
        }
        
        groupMemoryBarrier();
        barrier();

        //scatter keys into shared memory
        for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i){
            s_subgroupHistograms[_offsets[i]] = keys[i];
        }
        groupMemoryBarrier();
        barrier();

        // scatter keys into device memory
        for (uint i = gl_LocalInvocationID.x; i < BIN_PART_SIZE; i += gl_WorkGroupSize.x){
            b_alt[s_localHistograms[s_subgroupHistograms[i] >> radix_shift & RADIX_MASK] + i] = s_subgroupHistograms[i];
        }
        
    }
    

    
    // process final partition
    if (partitionIndex == gl_NumWorkGroups.x - 1){
        groupMemoryBarrier();
        barrier();

        if (partitionIndex > 0){
            // lookback
            if(gl_LocalInvocationID.x < RADIX_BIN){
                uint reduction = 0;    
                for (int k = int(partitionIndex); k > 0;){
                    const uint flagPayload = b_passHist[gl_LocalInvocationID.x * gl_NumWorkGroups.x + k - 1];

                    if ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE){
                        reduction += flagPayload >> 2;
                        s_localHistograms[gl_LocalInvocationID.x] += reduction;
                        break;
                    }

                    if ((flagPayload & FLAG_MASK) == FLAG_AGGREGATE){
                        reduction += flagPayload >> 2;
                        k--;
                    }
                }     
            
            }
            groupMemoryBarrier();
            barrier();
        }
    
        
        const uint partEnd = BIN_PART_START + BIN_PART_SIZE;
        for (uint i = gl_LocalInvocationID.x + BIN_PART_START; i < partEnd; i += gl_WorkGroupSize.x){
            uint key;
            uint offset;
            uint warpFlags = 0xFFFFFFFF;

            if ( i < input_size)
                key = b_sort[i];
            
            for (uint k = 0; k < RADIX_LOG; ++k){
                const bool t = (key >> k + radix_shift & 1) > 0;
                warpFlags &= (t ? 0 : 0xFFFFFFFF) ^ subgroupBallot(t).x;
            }

            const uint bits = subgroupBallotExclusiveBitCount(uvec4(warpFlags,0,0,0));

            for (uint k = 0; k < BIN_SUBGROUPS; ++k){
                uint prev;
                if (gl_SubgroupID.x == k && bits == 0 && i < input_size){
                    prev = atomicAdd(s_localHistograms[key >> radix_shift & RADIX_MASK], bitCount(warpFlags));
                }
                if (gl_SubgroupID.x == k){
                    offset = subgroupShuffle(prev, findLSB(warpFlags)) + bits;
                }
                groupMemoryBarrier();
                barrier();
            }

            if (i < input_size)
                b_alt[offset] = key;
        }
        
    }
}