#version 450
#extension GL_KHR_shader_subgroup_basic: enable
#extension GL_KHR_shader_subgroup_ballot: enable
#extension GL_KHR_shader_subgroup_arithmetic: enable
#extension GL_KHR_shader_subgroup_vote: enable
#extension  GL_EXT_shader_explicit_arithmetic_types_int64 : enable
#extension GL_NV_shader_atomic_int64 : enable
#extension GL_EXT_shader_subgroup_extended_types_int64 : enable

#extension GL_EXT_debug_printf : require

#define input_size 7680

#define RADIX_BIN 256
#define RADIX_LOG 8
#define RADIX_BITS 8
#define RADIX_MASK 255 // Mask of digit bins
#define RADIX_PASS 4//(sizeof(uint) * 8 + RADIX_BITS - 1) / RADIX_BITS
#define SEC_RADIX           8       //Shift value to retrieve digits from the second place
#define THIRD_RADIX         16      //Shift value to retrieve digits from the third place
#define FOURTH_RADIX        24      //Shift value to retrieve digits from the fourth place 
#define SEC_RADIX_START     256     //Offset for retrieving values from global buffer
#define THIRD_RADIX_START   512     //Offset for retrieving values from global buffer
#define FOURTH_RADIX_START  768     //Offset for retrieving values from global buffer

#define LANE_COUNT 64 // number of threads in a subgroup
#define LANE_MASK 63
#define LANE_LOG 6

#define LANE gl_LocalInvocationID.x // the idx of thread in the subgroup
#define SUBGROUP_IDX  gl_LocalInvocationID.y//gl_SubgroupID // the idx of subgroup the thread belongs to might be wrong
#define SUBGROUP_THREAD_IDX gl_GlobalInvocationID.x //(LANE + (SUBGROUP_IDX << LANE_LOG)) // the subgroup relative thread idx                                 



#define HIST_SUBGROUP                                                           \
  8 // number of subgroups in a thread block/work group for executing histogram
    // kernel
#define HIST_THREADS                                                           \
  512 // number of threads in a thread block/work group for executing histogram
     // kernel
#define HIST_TBLOCKS                                                           \
  2 // number of thread blocks/workgroups for executing histogram kernel

#define HIST_PART_SIZE (input_size / HIST_TBLOCKS) // the size of each partition
#define HIST_PART_START                                                        \
  (gl_WorkGroupID.x * HIST_PART_SIZE) // the start index of each partition. work
                                    // group id in vulkan, block idx in cuda

#define HIST_PART_END                                                          \
  (gl_WorkGroupID.x == HIST_TBLOCKS - 1 ? input_size                             \
                                      : (gl_WorkGroupID.x + 1) * HIST_PART_SIZE)



//For the binning
#define BIN_PART_SIZE       7680    //The partition tile size of a BinningPass threadblock
#define BIN_HISTS_SIZE      4096    //The total size of all subgroup histograms in shared memory
#define BIN_TBLOCKS         2     //The number of threadblocks dispatched in a BinningPass threadblock
#define BIN_THREADS         512     //The number of threads in a BinningPass threadblock
#define BIN_SUB_PART_SIZE   960     //The subpartition tile size of a single subgroup in a BinningPass threadblock
#define BIN_SUBGROUPS       8       //The number of subgroup in a BinningPass threadblock previously 16
#define BIN_KEYS_PER_THREAD 15      //The number of keys per thread in BinningPass threadblock previously 15

#define BIN_PARTITIONS     (input_size / BIN_PART_SIZE)             //The number of partition tiles in a BinningPass
#define BIN_SUB_PART_START (SUBGROUP_IDX * BIN_SUB_PART_SIZE)     //The starting offset of a subpartition tile
#define BIN_PART_START     (partitionIndex * BIN_PART_SIZE)     //The starting offset of a partition tile



#define FLAG_NOT_READY      0       //Flag value inidicating neither inclusive sum, or aggregate sum of a partition tile is ready
#define FLAG_AGGREGATE      1       //Flag value indicating aggregate sum of a partition tile is ready
#define FLAG_INCLUSIVE      2       //Flag value indicating inclusive sum of a partition tile is ready
#define FLAG_MASK           3       //Mask used to retrieve flag values



layout(set = 0, binding = 0)  buffer BSortBuffer {
    uint b_sort[15360];
};

layout(set = 0, binding = 1)  buffer BAltBuffer {
    uint b_alt[15360];
};

layout(set = 0, binding = 2)  buffer BGlobalHist {
    uint b_globalHist[1024];
};

layout(set = 0, binding = 3) coherent buffer BIndex {
    uint b_index[4];
};

layout(set = 0, binding = 4) coherent buffer BPassHist {
    uvec4 b_passHist[512];
};

layout(set = 0, binding = 5) buffer OutReduction{
    uint out_reduction[RADIX_BIN];
};


// define work group size
layout(local_size_x = 64, local_size_y = 8) in;
shared uvec4 g_globalHist[RADIX_BIN];
shared uint g_localHist[RADIX_BIN];
shared uint g_subgroupHists[BIN_PART_SIZE];
shared uint g_reductionHist[RADIX_BIN];

//[numthreads(LANE_COUNT, G_HIST_WAVES, 1)]

void GlobalHistogram() {
  // initialization
  for (uint i = SUBGROUP_THREAD_IDX; i < RADIX_BIN; i += HIST_THREADS) {
    g_globalHist[i][0] = 0;
    g_globalHist[i][1] = 0;
    g_globalHist[i][2] = 0;
    g_globalHist[i][3] = 0;
  }
  groupMemoryBarrier();
  barrier();

  // histogram
  // add number of occurence of each 1 at different digit place to global histogram
  // there are 8 digits place for each pass, so we have 8*4 digits place in total for 32 bits integer
  const uint partitionEnd = HIST_PART_END;
  for (uint i = SUBGROUP_THREAD_IDX + HIST_PART_START; i < partitionEnd;
       i += HIST_THREADS) {
    const uint key = b_sort[i];
    atomicAdd(g_globalHist[key & RADIX_MASK][0], 1);
    atomicAdd(g_globalHist[(key >> RADIX_LOG) & RADIX_MASK][1], 1);
    atomicAdd(g_globalHist[(key >> (2 * RADIX_LOG)) & RADIX_MASK][2], 1);
    atomicAdd(g_globalHist[(key >> (3 * RADIX_LOG)) & RADIX_MASK][3], 1);
  }
  groupMemoryBarrier();
  barrier();

  // prefix_sum
  // prefix sum at warp/subgroup/wave level
  // scan occurence of digits containg digit 1 in the first i+1 bins for each warp/subgroup/wave, and store the result in g_globalHist
  for (uint i = SUBGROUP_IDX << LANE_LOG; i < RADIX_BIN; i += HIST_THREADS) {
    g_globalHist[((LANE + 1) & LANE_MASK) + i][0] = subgroupExclusiveAdd(g_globalHist[LANE + i][0]) +  g_globalHist[LANE + i][0];
    g_globalHist[((LANE + 1) & LANE_MASK) + i][1] = subgroupExclusiveAdd(g_globalHist[LANE + i][1]) +  g_globalHist[LANE + i][1];
    g_globalHist[((LANE + 1) & LANE_MASK) + i][2] = subgroupExclusiveAdd(g_globalHist[LANE + i][2]) +  g_globalHist[LANE + i][2];
    g_globalHist[((LANE + 1) & LANE_MASK) + i][3] = subgroupExclusiveAdd(g_globalHist[LANE + i][3]) +  g_globalHist[LANE + i][3];
  }
    groupMemoryBarrier();
    barrier();

  if ((LANE < (RADIX_BIN >> LANE_LOG)) && (SUBGROUP_IDX == 0)) {
    g_globalHist[LANE << LANE_LOG][0] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][0]);
    g_globalHist[LANE << LANE_LOG][1] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][1]);
    g_globalHist[LANE << LANE_LOG][2] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][2]);
    g_globalHist[LANE << LANE_LOG][3] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][3]);
  }
    groupMemoryBarrier();
    barrier();

    uint k = SUBGROUP_THREAD_IDX;
    
    // prefixsum at global level
    // b_globalHist holds global bin offset, it is used to indicate where each block can begin scattering its keys into the different digit bins
    // for example, b_globalHist[255] = 100 meanning that there are 100 keys that contain digit 1 in the first 255 bins(least signifcant 8 bits)
    const bool is_not_first_lane = (LANE != 0);
    const bool is_not_first_subgroup = (SUBGROUP_IDX != 0);
    atomicAdd(b_globalHist[k], (is_not_first_lane ? g_globalHist[k][0] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][0], 0) : 0));
    atomicAdd(b_globalHist[k + RADIX_BIN], (is_not_first_lane ? g_globalHist[k][1] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][1], 0) : 0));
    atomicAdd(b_globalHist[k + RADIX_BIN*2], (is_not_first_lane ? g_globalHist[k][2] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][2], 0) : 0));
    atomicAdd(b_globalHist[k + RADIX_BIN*3], (is_not_first_lane ? g_globalHist[k][3] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][3], 0) : 0));

    for (k += HIST_THREADS; k < RADIX_BIN; k += HIST_THREADS)
    {
        atomicAdd(b_globalHist[k], (is_not_first_lane ? g_globalHist[k][0] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][0], 0));
        atomicAdd(b_globalHist[k + RADIX_BIN], (is_not_first_lane ? g_globalHist[k][1] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][1], 0));
        atomicAdd(b_globalHist[k + RADIX_BIN*2], (is_not_first_lane ? g_globalHist[k][2] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][2], 0));
        atomicAdd(b_globalHist[k + RADIX_BIN*3], (is_not_first_lane ? g_globalHist[k][3] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][3], 0));
    }
    
}


void BinningPass(int pass_num)
{
    //debugPrintfEXT("binning: b_sort[%u]: %u\n", 0, b_sort[0]);
   // debugPrintfEXT("global invocation id: %u\n sub group id: %u\n sub group invocation id: %u\n workgroup id: %u\n subgroup_threads_idx: %u\n\n",  
    //    gl_GlobalInvocationID.x, gl_SubgroupID, gl_SubgroupInvocationID,  gl_WorkGroupID.x, SUBGROUP_THREAD_IDX);
    //load the global histogram values into shared memory

    if (SUBGROUP_THREAD_IDX < RADIX_BIN){
        g_localHist[SUBGROUP_THREAD_IDX] = b_globalHist[SUBGROUP_THREAD_IDX + RADIX_BIN * pass_num];
        //debugPrintfEXT("g_localHist[%u]: %u\n", SUBGROUP_THREAD_IDX, g_localHist[SUBGROUP_THREAD_IDX]);
    }
    
    //atomically fetch and increment device memory index to assign partition tiles
    // g_localHist[0] is set to be the original value of b_index[0]
    if (LANE == 0 && SUBGROUP_IDX == 0)
        g_localHist[0] = atomicAdd(b_index[pass_num], 1);
    groupMemoryBarrier();
    barrier();
    // Returns the value of the local histogram at bin 0 for the given lane index within the specified subgroup.
    // it returns the current partitionIndex 
    // in our case, paritionIndex is the index of the current partition tile(index of workgroup)
    uint partitionIndex = subgroupBroadcast(g_localHist[0], 0);


    for (uint i = SUBGROUP_THREAD_IDX; i < BIN_HISTS_SIZE; i += BIN_THREADS)
        g_subgroupHists[i] = 0;
    groupMemoryBarrier();
    barrier();
    
    //Load keys into registers
    uint keys[BIN_KEYS_PER_THREAD];
    
    //[unroll]
    for (uint i = 0, t = uint(LANE + BIN_SUB_PART_START + BIN_PART_START); i < BIN_KEYS_PER_THREAD; ++i, t += LANE_COUNT){
        if (pass_num %2 == 0){
            keys[i] = b_sort[t];
            if(pass_num != 0){
                //debugPrintfEXT("b_sort[%u]: %u\n", t, b_sort[t]);
            }
        }else{
            keys[i] = b_alt[t];
            if(t == 180){
                debugPrintfEXT("b_alt[%u]: %u\n", t, b_alt[t]);
            }
        }
    }


    // [ 2, 5, 6 , 1]
    // output= [1, 2, 3, 0]
    // [0, 2, 7, 13, 14]
    //Warp Level Multisplit
    // this step ranks the key in warp-level by computes both 
    // It determines the correct per-thread offset if threads share a key value.
    // It determines the total count of a given key value in a warp, and safely increments the histogram in shared memory.
    
    uint64_t offsets[BIN_KEYS_PER_THREAD];
    
        const uint t = SUBGROUP_IDX << RADIX_LOG;
            
        //[unroll]
        for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i)
        {   
            offsets[i] = 0xFFFFFFFFFFFFFFFFL;

            //    [unroll]
            for (int k = RADIX_LOG * pass_num; k < RADIX_LOG * (pass_num + 1); ++k)
            {
                // extracts the k-th bit of the key, t2 is true if the k-th bit of keys[i] is 1, and false otherwise.
                const bool t2 = (keys[i] >> k & 1u) != 0u;
                const uvec4 ballot = subgroupBallot(t2);
                uint64_t v = (uint64_t(ballot.x)) ^ (uint64_t(ballot.y) << 32);
                // subgroupActiveBallot counts the number of thread in the subgroup that have the k-th bit of keys[i] set to 1
                // if t2 is true, then 0 xor subgroupActiveBallot(t2), oherwise 0xFFFFFFFF xor subgroupActiveBallot(t2), we get inverse result for t2 = true and false
                offsets[i] &= (t2 ? 0 : 0xFFFFFFFFFFFFFFFFL) ^ v;//(subgroupBallot(t2).x);
            }
            
            // Number of peers having same digit as key[i]
            const uint bits = uint(bitCount(offsets[i] << LANE_MASK - LANE));
            

            int index = 0;
            if(pass_num == 3){
                index = int(keys[i] >> 24 /*+ t*/);
            }else{
                index = int((keys[i] >> (RADIX_LOG * pass_num) & RADIX_MASK) /*+ t*/);
            }
            const uint64_t prev = g_subgroupHists[index];
            
            // total count of  key[i] value in a warp
            if (bits == 1)
                g_subgroupHists[index] += uint(bitCount(offsets[i]));
            // Number of prior keys having same digit
            offsets[i] = prev + bits - 1;
        }
    groupMemoryBarrier();
    barrier();
    // block level exclusive prefix sum across the histograms
    // This provides (1) tile-wide digit counts for participating in chained scan cooperation with other blocks, and 
    // (2) tile-relative bin offsets for each warp to scatter its elements into local bins within shared memory.
    if (SUBGROUP_THREAD_IDX < RADIX_BIN)
    {
        
        for (int k = int(SUBGROUP_THREAD_IDX + RADIX_BIN); k < BIN_HISTS_SIZE; k += RADIX_BIN)
        {
            g_subgroupHists[SUBGROUP_THREAD_IDX] += g_subgroupHists[k];
            g_subgroupHists[k] = g_subgroupHists[SUBGROUP_THREAD_IDX] - g_subgroupHists[k];
        }
        
        g_reductionHist[SUBGROUP_THREAD_IDX] = g_subgroupHists[SUBGROUP_THREAD_IDX];
        
        // partitionIndex == 0 means it is the first partition, so  the value is the inclusive global prefix sum of the digit counts of this and all previous tiles
        if (partitionIndex == 0){
            atomicAdd(b_passHist[uint(SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex)][pass_num], FLAG_INCLUSIVE ^ (g_subgroupHists[SUBGROUP_THREAD_IDX] << 2));
        }
        else{
        // otherwsie,  value is the local, per-tile digit count
            atomicAdd(b_passHist[uint(SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex)][pass_num], FLAG_AGGREGATE ^ (g_subgroupHists[SUBGROUP_THREAD_IDX] << 2));
        }

    }
    memoryBarrierBuffer();
    barrier(); // a barrier must be placed between here and the lookback to prevent accidentally broadcasting an incorrect aggregate
    
    //exclusive prefix sum across the reductions
    if (SUBGROUP_THREAD_IDX < RADIX_BIN)
        g_reductionHist[((LANE + 1) & LANE_MASK) + (SUBGROUP_IDX << LANE_LOG)] = subgroupExclusiveAdd(g_reductionHist[SUBGROUP_THREAD_IDX]) + g_reductionHist[SUBGROUP_THREAD_IDX];
    memoryBarrierBuffer();
    barrier();
        
    if (LANE < (RADIX_BIN >> LANE_LOG) && SUBGROUP_IDX == 0)
        g_reductionHist[LANE << LANE_LOG] = subgroupExclusiveAdd(g_reductionHist[LANE << LANE_LOG]);
    memoryBarrierBuffer();
    barrier();

    if ((SUBGROUP_THREAD_IDX < RADIX_BIN) && (LANE != 0))
        g_reductionHist[SUBGROUP_THREAD_IDX] += subgroupBroadcast(g_reductionHist[SUBGROUP_THREAD_IDX - 1], 1);
    memoryBarrierBuffer();
    barrier();
    //
    //Update offsets
    if (SUBGROUP_IDX != 0)
    {
        //const uint t = SUBGROUP_IDX << RADIX_LOG;

        if (pass_num == 3){
            for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            {
                const uint t2 = keys[i] >> 24;
                offsets[i] += g_subgroupHists[t2 /*+ t*/] + g_reductionHist[t2];
            }
        }else{   
        //[unroll]
            for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            {
                const uint t2 = keys[i] >> (RADIX_LOG * pass_num) & RADIX_MASK;
                offsets[i] += g_subgroupHists[t2 /*+ t*/] + g_reductionHist[t2];
            }
        }
    }
    
    else
    {
        //[unroll]
        if (pass_num == 3){
        for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            offsets[i] += g_reductionHist[keys[i] >> 24];
        }else{
            for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            offsets[i] += g_reductionHist[keys[i] >> (RADIX_LOG * pass_num) & RADIX_MASK];
        }
    }
    
    groupMemoryBarrier();
    barrier();

    /*
    if (pass_num == 3){
                debugPrintfEXT("offsets[%u]: %u, offsets[%u]: %u \n", 0, offsets[0], 1, offsets[1]);
    }
    */
    
    //Scatter keys into shared memory
    // now the g_subgroupHists contain the offset of each scattered key in the subgroup
    
    for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i){
        g_subgroupHists[uint(offsets[i])] = keys[i];
    }
    groupMemoryBarrier();
    barrier();
    
    if( pass_num != 0){
        for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i){
            debugPrintfEXT("offsets[%u]: %u \t keys[%u]: %u\n", i, offsets[i], i, keys[i]);
        }
    }
    
    
    // The results are correct up to this point
    //Lookback
    // block 31 scan = block 30 scan result + current block prefix sum
    // block 31 = block 29 scan result + block 30 prefix sum + current block prefix sum
    if (partitionIndex != 0)
    {
        //Free up shared memory, because we are at max
        // set t to be scattered key.
        const uint t = g_subgroupHists[SUBGROUP_IDX];
       // debugPrintfEXT("global invocation id: %u\n sub group id: %u\n sub group invocation id: %u\n workgroup id: %u\n ",  
        //    gl_GlobalInvocationID.x, gl_SubgroupID, gl_SubgroupInvocationID,  gl_WorkGroupID.x);
        // for loop at block level
        //debugPrintfEXT("partitionIndex - LANE - 1: %d\n", int(partitionIndex - LANE - 1));
        for (int i = int(SUBGROUP_IDX); i < RADIX_BIN; i += BIN_SUBGROUPS)
        {
            uint aggregate = 0;

            // set the prefix sum for current subgroup to be the partitionIndex
            if (LANE == 0)
                g_subgroupHists[SUBGROUP_IDX] = partitionIndex;
            // Within the same block, look backward progressively to find the predecessors that record the global inclusive prefix up to and including current tile.
            // todo: delete the lane-count
            //if (partitionIndex -1 >= LANE){
            for (int k = int(partitionIndex - LANE - 1); 0 <= k;)
            {
                
                //debugPrintfEXT("before: %d, after: %d\n", SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex, i * BIN_PARTITIONS + k);
                // get the reduction sum on the predecessors bin in current block
                uint flagPayload = b_passHist[i * BIN_PARTITIONS + k][pass_num];
                // if the reduction sum on the predecessors bin is ready for all threads in current subgroup
                //debugPrintfEXT("i * BIN_PARTITIONS + k = %u, flagPayload & FLAG_MASK  > FLAG_NOT_READY: %d\n", i * BIN_PARTITIONS + k,  (flagPayload & FLAG_MASK) > FLAG_NOT_READY);
                if (subgroupAll((flagPayload & FLAG_MASK) > FLAG_NOT_READY))
                {
                    
                    //debugPrintfEXT("flagPayload & FLAG_MASK  > FLAG_NOT_READY: %d\n", (flagPayload & FLAG_MASK) > FLAG_NOT_READY);
                    
                    
                    // if the predecessors record the global inclusive prefix up to and including current tile
                    if ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE)
                    {   
                        //debugPrintfEXT("okkk\n" );
                        
                        // if current lane has smallest index in the subgroup, then simply set the value to k as index k indicates that  represent the global inclusive prefix sum
                        if (subgroupElect()){
                            g_subgroupHists[SUBGROUP_IDX] = k;

                        }
                        
                            
                    }
                    //debugPrintfEXT("g_subgroupHists[%u]: %u, k: %d\n", SUBGROUP_IDX, g_subgroupHists[SUBGROUP_IDX], k);
                    
                    // if the prefix sum of current subgroup is less than partitionIndex, which means we set g_subgroupHists[SUBGROUP_IDX] to be k in the previous if statement
                    // which means the current subgroup hist record the global inclusive prefix up to and including current tile.
                    if (g_subgroupHists[SUBGROUP_IDX] < partitionIndex)
                    {
                        // sum up the prefix sum of each lane in the subgroup that is ready (which we set to be k in the previous if statement)
                        aggregate += subgroupAdd(k >= g_subgroupHists[SUBGROUP_IDX] ? (flagPayload >> 2) : 0);
                        
                        // if current lane is the first lane in the subgroup
                        if (LANE == 0)
                        {
                            //  then we  add the aggregate sum to the reduction sum of current block and end the lookback for current block
                            atomicAdd(b_passHist[i * BIN_PARTITIONS + partitionIndex][pass_num], 1 ^ (aggregate << 2));
                            // if the subgroup is not the first subgroup, then we add the aggregate sum to the global prefix sum until the previous subgroup
                            g_reductionHist[i] = aggregate + ((i != 0) ? g_localHist[i] : 0) - g_reductionHist[i];
                        }
                        break;
                    }
                    // if the prefix sum of current subgroup is greater or equal to partitionIndex,
                    // which means the current subgroup hist contains the aggregate prefix sum of the k-th bin in current block
                    // 
                    else
                    {
                        //debugPrintfEXT("partitionIndex: %u, g_subgroupHists[%u]: %u, k: %d\n", partitionIndex, SUBGROUP_IDX, g_subgroupHists[SUBGROUP_IDX], k);
                        aggregate += subgroupAdd(flagPayload >> 2);
                        k -= LANE_COUNT;
                    }
                    
                }
            }
       // }
            
        }
            
        //place value back
        if (LANE == 0)
            g_subgroupHists[SUBGROUP_IDX] = t; 
    }
    
    else
    {
        if (SUBGROUP_THREAD_IDX < RADIX_BIN)
            g_reductionHist[SUBGROUP_THREAD_IDX] = (SUBGROUP_THREAD_IDX != 0 ? g_localHist[SUBGROUP_THREAD_IDX] : 0) - g_reductionHist[SUBGROUP_THREAD_IDX];
    }
    groupMemoryBarrier();
    barrier();
    
    //Scatter runs of keys into device memory;
   // if(pass_num == 0)
   // debugPrintfEXT("g_subgroupHists[10]: %u\n", g_subgroupHists[10]);
    if (pass_num == 3){
        // fuck it is always 0 for g_subgroupHists[i];
        for (uint i = SUBGROUP_THREAD_IDX; i < BIN_PART_SIZE; i += BIN_THREADS){
            b_sort[g_reductionHist[g_subgroupHists[i] >> 24] + i] = g_subgroupHists[i];
        }
    }else{
        for (uint i = SUBGROUP_THREAD_IDX; i < BIN_PART_SIZE; i += BIN_THREADS){
            if (pass_num == 0 || pass_num == 2){
                b_alt[g_reductionHist[g_subgroupHists[i] >> (RADIX_LOG * pass_num) & RADIX_MASK] + i] = g_subgroupHists[i];
            }else{
                b_sort[g_reductionHist[g_subgroupHists[i] >> (RADIX_LOG * pass_num) & RADIX_MASK] + i] = g_subgroupHists[i];
                                if( g_reductionHist[g_subgroupHists[i] >> (RADIX_LOG * pass_num) & RADIX_MASK] + i <= 10)
                debugPrintfEXT("b_alt[%u]: %u\n", g_reductionHist[g_subgroupHists[i] >> (RADIX_LOG * pass_num) & RADIX_MASK] + i, g_subgroupHists[i]);
            }
        }
    }

    
    //for input sizes which are not perfect multiples of the partition tile size
    if (partitionIndex == BIN_PARTITIONS - 1)
    {
        if (SUBGROUP_THREAD_IDX < RADIX_BIN)
            g_reductionHist[SUBGROUP_THREAD_IDX] = (b_passHist[SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex][pass_num] >> 2) + (SUBGROUP_THREAD_IDX != 0 ? g_localHist[SUBGROUP_THREAD_IDX] : 0);
        groupMemoryBarrier();
        barrier();
        
        partitionIndex++;
        for (uint i = SUBGROUP_THREAD_IDX + BIN_PART_START; i < input_size; i += BIN_THREADS)
        {
            const uint key = (pass_num %2 == 0 ? b_sort[i] : b_alt[i]);
            uint64_t offset = 0xFFFFFFFFFFFFFFFFL;
            
           // [unroll]
            for (uint k = pass_num * RADIX_LOG; k < ((pass_num + 1) * RADIX_LOG); ++k)
            {
                const bool t = (key >> k & 1) != 0;
                const uvec4 ballot = subgroupBallot(t);
                uint64_t v = (uint64_t(ballot.x)) ^ (uint64_t(ballot.y) << 32);
                offset &= (t ? 0 : 0xFFFFFFFFFFFFFFFFL) ^ v;
            }
            
            //[unroll]
            for (int k = 0; k < BIN_SUBGROUPS; ++k)
            {
                if (SUBGROUP_IDX == k)
                {
                    if(pass_num == 3){
                        const uint t = g_reductionHist[(key >> 24)];
                        if (bitCount(offset << (LANE_MASK - LANE)) == 1)
                            g_reductionHist[key >> 24] += uint(bitCount(offset));
                        offset = t + bitCount((offset << (LANE_MASK - LANE)) << 1);
                    }else{
                        const uint t = g_reductionHist[key >> (RADIX_LOG*pass_num) & RADIX_MASK];
                        if (bitCount(offset << (LANE_MASK - LANE)) == 1)
                            g_reductionHist[key >> (RADIX_LOG*pass_num) & RADIX_MASK] += uint(bitCount(offset));
                        offset = t + bitCount((offset << (LANE_MASK - LANE)) << 1);
                    }

                }
                groupMemoryBarrier();
                barrier();
            }
            if (pass_num % 2 == 0){
                b_alt[uint(offset)] = key;
            }else{
                b_sort[uint(offset)] = key;
            }
        }
    }
    
    
}

void initialization(){
  for(int i = 0; i < 1024; i++){
    b_globalHist[i] = 0;
  }
  for(int i = 0; i < 4; i++){
    b_index[i] = 0;
  }
  for(int i = 0; i <512; i++){
    b_passHist[i] = uvec4(0, 0, 0, 0);

  }
  for( int i = 0; i < RADIX_BIN; i++){
    g_reductionHist[i] = 0;
    g_localHist[i] = 0;
    g_globalHist[i][0] = 0;
    g_globalHist[i][1] = 0;
    g_globalHist[i][2] = 0;
    g_globalHist[i][3] = 0;
  }
  for( int i = 0; i < BIN_PART_SIZE; i++){
    g_subgroupHists[i] = 0;
  }
  for(int i = 0; i < input_size; i++){
    b_alt[i] = 0;
  }
}

void finalize(){
    if (gl_WorkGroupID.x == 0){
        for(int i = 0; i < RADIX_BIN; i++){
            out_reduction[i] = g_reductionHist[i];
        }
    }
}

void main(){
    if (gl_GlobalInvocationID.x == 0) {
        debugPrintfEXT("subgroups %u, subgroup_threads: %u\n", gl_NumSubgroups, gl_SubgroupSize);

    }
  initialization();
  groupMemoryBarrier();
  barrier();
  GlobalHistogram();
  groupMemoryBarrier();
  barrier();
  
  BinningPass(0);
  groupMemoryBarrier();
  barrier();
  
  BinningPass(1);
  groupMemoryBarrier();
  barrier();
  
  finalize();
  /*
  groupMemoryBarrier();
  barrier();
    
  BinningPass(2);
  groupMemoryBarrier();
  barrier();
  BinningPass(3);
  groupMemoryBarrier();
  barrier();
*/
  //barrier();
  //test();
  /*
  barrier();
  BinningPass(1);
  barrier();
  BinningPass(2);
  barrier();
  BinningPass(3);
    */
}