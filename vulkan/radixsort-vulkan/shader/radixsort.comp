#version 450
#extension GL_KHR_shader_subgroup_basic: enable
#extension GL_KHR_shader_subgroup_ballot: enable
#extension GL_KHR_shader_subgroup_arithmetic: enable
#extension GL_KHR_shader_subgroup_vote: enable
#extension GL_EXT_debug_printf : require

#define input_size 15360

#define RADIX_BIN 256
#define RADIX_LOG 8
#define RADIX_BITS 8
#define RADIX_MASK 255 // Mask of digit bins
#define RADIX_PASS 4//(sizeof(uint) * 8 + RADIX_BITS - 1) / RADIX_BITS

#define LANE_COUNT 32 // number of threads in a subgroup
#define LANE_MASK 31
#define LANE_LOG 5

#define HIST_SUBGROUP                                                           \
  16 // number of subgroups in a thread block/work group for executing histogram
    // kernel
#define HIST_THREADS                                                           \
  512 // number of threads in a thread block/work group for executing histogram
     // kernel
#define HIST_TBLOCKS                                                           \
  2 // number of thread blocks/workgroups for executing histogram kernel

#define HIST_PART_SIZE (input_size / HIST_TBLOCKS) // the size of each partition
#define HIST_PART_START                                                        \
  (gl_WorkGroupID.x * HIST_PART_SIZE) // the start index of each partition. work
                                    // group id in vulkan, block idx in cuda

#define HIST_PART_END                                                          \
  (gl_WorkGroupID.x == HIST_TBLOCKS - 1 ? input_size                             \
                                      : (gl_WorkGroupID.x + 1) * HIST_PART_SIZE)



//For the binning
#define BIN_PART_SIZE       7680    //The partition tile size of a BinningPass threadblock
#define BIN_HISTS_SIZE      4096    //The total size of all subgroup histograms in shared memory
#define BIN_TBLOCKS         2     //The number of threadblocks dispatched in a BinningPass threadblock
#define BIN_THREADS         512     //The number of threads in a BinningPass threadblock
#define BIN_SUB_PART_SIZE   480     //The subpartition tile size of a single subgroup in a BinningPass threadblock
#define BIN_SUBGROUPS       16       //The number of subgroup in a BinningPass threadblock previously 16
#define BIN_KEYS_PER_THREAD 15      //The number of keys per thread in BinningPass threadblock previously 15

#define BIN_PARTITIONS     (input_size / BIN_PART_SIZE)             //The number of partition tiles in a BinningPass
#define BIN_SUB_PART_START (SUBGROUP_IDX * BIN_SUB_PART_SIZE)     //The starting offset of a subpartition tile
#define BIN_PART_START     (partitionIndex * BIN_PART_SIZE)     //The starting offset of a partition tile


#define LANE gl_LocalInvocationID.x // the idx of thread in the subgroup
#define SUBGROUP_IDX  gl_LocalInvocationID.y//gl_SubgroupID // the idx of subgroup the thread belongs to might be wrong
#define SUBGROUP_THREAD_IDX                                                             \
  (LANE + (SUBGROUP_IDX << LANE_LOG)) // the subgroup relative thread idx


#define FLAG_NOT_READY      0       //Flag value inidicating neither inclusive sum, or aggregate sum of a partition tile is ready
#define FLAG_AGGREGATE      1       //Flag value indicating aggregate sum of a partition tile is ready
#define FLAG_INCLUSIVE      2       //Flag value indicating inclusive sum of a partition tile is ready
#define FLAG_MASK           3       //Mask used to retrieve flag values



layout(set = 0, binding = 0)  buffer BSortBuffer {
    uint b_sort[];
};

layout(set = 0, binding = 1)  buffer BAltBuffer {
    uint b_alt[];
};

layout(set = 0, binding = 2)  buffer BGlobalHist {
    uint b_globalHist[];
};

layout(set = 0, binding = 3) coherent buffer BIndex {
    uint b_index[];
};

layout(set = 0, binding = 4) coherent buffer BPassHist {
    uvec4 b_passHist[input_size];
};


// define work group size
layout(local_size_x = 32, local_size_y = 16) in;
shared uint g_globalHist[RADIX_BIN][RADIX_PASS];
shared uint g_localHist[RADIX_BIN];
shared uint g_subgroupHists[BIN_PART_SIZE];
shared uint g_reductionHist[RADIX_BIN];

//[numthreads(LANE_COUNT, G_HIST_WAVES, 1)]

void GlobalHistogram() {
  // initialization
  for (uint i = SUBGROUP_THREAD_IDX; i < RADIX_BIN; i += HIST_THREADS) {
    g_globalHist[i][0] = 0;
    g_globalHist[i][1] = 0;
    g_globalHist[i][2] = 0;
    g_globalHist[i][3] = 0;
  }
  groupMemoryBarrier();
  barrier();

  // histogram
  // add number of occurence of each 1 at different digit place to global histogram
  // there are 8 digits place for each pass, so we have 8*4 digits place in total for 32 bits integer
  const uint partitionEnd = HIST_PART_END;
  for (uint i = SUBGROUP_THREAD_IDX + HIST_PART_START; i < partitionEnd;
       i += HIST_THREADS) {
    const uint key = b_sort[i];
    atomicAdd(g_globalHist[key & RADIX_MASK][0], 1);
    atomicAdd(g_globalHist[key >> RADIX_LOG & RADIX_MASK][1], 1);
    atomicAdd(g_globalHist[key >> (2 * RADIX_LOG) & RADIX_MASK][2], 1);
    atomicAdd(g_globalHist[key >> (3 * RADIX_LOG) & RADIX_MASK][3], 1);
  }
  groupMemoryBarrier();
  barrier();

  // prefix_sum
  // prefix sum at warp/subgroup/wave level
  // scan occurence of digits containg digit 1 in the first i+1 bins for each warp/subgroup/wave, and store the result in g_globalHist
  for (uint i = SUBGROUP_IDX << LANE_LOG; i < RADIX_BIN; i += HIST_THREADS) {
    g_globalHist[((LANE + 1) & LANE_MASK) + i][0] = subgroupExclusiveAdd(g_globalHist[LANE + i][0] +  g_globalHist[LANE + i][0]);
    g_globalHist[((LANE + 1) & LANE_MASK) + i][1] = subgroupExclusiveAdd(g_globalHist[LANE + i][1] +  g_globalHist[LANE + i][1]);
    g_globalHist[((LANE + 1) & LANE_MASK) + i][2] = subgroupExclusiveAdd(g_globalHist[LANE + i][2] +  g_globalHist[LANE + i][2]);
    g_globalHist[((LANE + 1) & LANE_MASK) + i][3] = subgroupExclusiveAdd(g_globalHist[LANE + i][3] +  g_globalHist[LANE + i][3]);
  //  g_globalHist[((LANE + 1) & LANE_MASK) + i] =
   //     SubGroupPrefixSum(g_globalHist[LANE + i]) + g_globalHist[LANE + i];
  }
    groupMemoryBarrier();
    barrier();

  if (LANE < (RADIX_BIN >> LANE_LOG) && SUBGROUP_IDX == 0) {
    g_globalHist[LANE << LANE_LOG][0] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][0]);
    g_globalHist[LANE << LANE_LOG][1] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][1]);
    g_globalHist[LANE << LANE_LOG][2] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][2]);
    g_globalHist[LANE << LANE_LOG][3] += subgroupExclusiveAdd(g_globalHist[LANE << LANE_LOG][3]);
  }
    groupMemoryBarrier();
    barrier();

    uint k = SUBGROUP_THREAD_IDX;
    
    // prefixsum at global level
    // b_globalHist holds global bin offset, it is used to indicate where each block can begin scattering its keys into the different digit bins
    // for example, b_globalHist[255] = 100 meanning that there are 100 keys that contain digit 1 in the first 255 bins(least signifcant 8 bits)
    const bool is_not_first_lane = (LANE != 0);
    const bool is_not_first_subgroup = (SUBGROUP_IDX != 0);
    atomicAdd(b_globalHist[k], (is_not_first_lane ? g_globalHist[k][0] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][0], 0) : 0));
    atomicAdd(b_globalHist[k + RADIX_BIN], (is_not_first_lane ? g_globalHist[k][1] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][1], 0) : 0));
    atomicAdd(b_globalHist[k + RADIX_BIN*2], (is_not_first_lane ? g_globalHist[k][2] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][2], 0) : 0));
    atomicAdd(b_globalHist[k + RADIX_BIN*3], (is_not_first_lane ? g_globalHist[k][3] : 0) + (is_not_first_subgroup ? subgroupBroadcast(g_globalHist[k - LANE_COUNT][3], 0) : 0));

    for (k += HIST_THREADS; k < RADIX_BIN; k += HIST_THREADS)
    {
        atomicAdd(b_globalHist[k], (is_not_first_lane ? g_globalHist[k][0] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][0], 0));
        atomicAdd(b_globalHist[k + RADIX_BIN], (is_not_first_lane ? g_globalHist[k][1] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][1], 0));
        atomicAdd(b_globalHist[k + RADIX_BIN*2], (is_not_first_lane ? g_globalHist[k][2] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][2], 0));
        atomicAdd(b_globalHist[k + RADIX_BIN*3], (is_not_first_lane ? g_globalHist[k][3] : 0) + subgroupBroadcast(g_globalHist[k - LANE_COUNT][3], 0));
    }
}


void BinningPass(int pass_num)
{
    //debugPrintfEXT("binning: b_sort[%u]: %u\n", 0, b_sort[0]);
   // debugPrintfEXT("global invocation id: %u\n sub group id: %u\n sub group invocation id: %u\n workgroup id: %u\n subgroup_threads_idx: %u\n\n",  
    //    gl_GlobalInvocationID.x, gl_SubgroupID, gl_SubgroupInvocationID,  gl_WorkGroupID.x, SUBGROUP_THREAD_IDX);
    //load the global histogram values into shared memory

    if (SUBGROUP_THREAD_IDX < RADIX_BIN){
        g_localHist[SUBGROUP_THREAD_IDX] = b_globalHist[SUBGROUP_THREAD_IDX + RADIX_BIN * pass_num];
        //debugPrintfEXT("g_localHist[%u]: %u\n", SUBGROUP_THREAD_IDX, g_localHist[SUBGROUP_THREAD_IDX]);
    }
    
    //atomically fetch and increment device memory index to assign partition tiles
    // g_localHist[0] is set to be the original value of b_index[0]
    if (LANE == 0 && SUBGROUP_IDX == 0)
        g_localHist[0] = atomicAdd(b_index[pass_num], 1);
    groupMemoryBarrier();
    barrier();
    // Returns the value of the local histogram at bin 0 for the given lane index within the specified subgroup.
    // it returns the current partitionIndex 
    int partitionIndex = int(subgroupBroadcast(g_localHist[0], 0));
    for (uint i = SUBGROUP_THREAD_IDX; i < BIN_HISTS_SIZE; i += BIN_THREADS)
        g_subgroupHists[i] = 0;
    groupMemoryBarrier();
    barrier();
    
    //Load keys into registers
    uint keys[BIN_KEYS_PER_THREAD];
    
    //[unroll]
    for (uint i = 0, t = LANE + BIN_SUB_PART_START + BIN_PART_START; i < BIN_KEYS_PER_THREAD; ++i, t += LANE_COUNT){
        if (pass_num %2 == 0){
            keys[i] = b_sort[t];
        }else{
            keys[i] = b_alt[t];
        }
    }
    //debugPrintfEXT("binning: b_sort[%u]: %u\n", 0, b_sort[0]);

    // [ 2, 5, 6 , 1]
    // output= [1, 2, 3, 0]
    // [0, 2, 7, 13, 14]
    //Warp Level Multisplit
    // this step ranks the key in warp-level by computes both 
    // (1) a warp-wide histogram of digit-counts, and (2) the warp-relative digit prefix count for each key
    
    uint offsets[BIN_KEYS_PER_THREAD];
    
        const uint t = SUBGROUP_IDX << RADIX_LOG;
            
        //[unroll]
        for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i)
        {   
            // fill with 1 so we can do ascending sort
            offsets[i] = 0xFFFFFFFF;

            //    [unroll]
            for (int k = RADIX_LOG * pass_num; k < RADIX_LOG * (pass_num + 1); ++k)
            {
                // extracts the k-th bit of the key, t2 is true if the k-th bit of keys[i] is 1, and false otherwise.
                const bool t2 = (keys[i] >> k & 1) != 0;
                // subgroupActiveBallot counts the number of thread in the subgroup that have the k-th bit of keys[i] set to 1
                // if t2 is true, then 0 xor subgroupActiveBallot(t2), oherwise 0xFFFFFFFF xor subgroupActiveBallot(t2), we get inverse result for t2 = true and false
                offsets[i] &= (t2 ? 0 : 0xFFFFFFFF) ^ (subgroupBallot(t2).x);
            }
            
            // counts the number of bits the current lane is responsible for, i.e. in a subgroup of 32 threads, 
            // if the current lane id is 0, then it is responsible for 1 bits, if it is 1, then it is responsible for 2 bits, and so on
            const uint bits = bitCount(offsets[i] << LANE_MASK - LANE);
            // index = least significant 8 bits of keys[i] + subgroup id * radix bin
            int index = 0;
            if(pass_num == 3){
                index = int(keys[i] >> 24 + t);
            }else{
                index = int((keys[i] >> (RADIX_LOG * pass_num) & RADIX_MASK) + t);
            }
            const uint prev = g_subgroupHists[index];
            //the lowest ranked thread in each active digit
            // population will  add the populationâ€™s count to the per-warp counter corresponding to that digit
            if (bits == 1)
                g_subgroupHists[index] += bitCount(offsets[i]);
            // updated to reflect the position where the i-th key should be placed in the sorted array.
            offsets[i] = prev + bits - 1;
        }
    groupMemoryBarrier();
    barrier();
    // block level exclusive prefix sum across the histograms
    // This provides (1) tile-wide digit counts for participating in chained scan cooperation with other blocks, and 
    // (2) tile-relative bin offsets for each warp to scatter its elements into local bins within shared memory.
    if (SUBGROUP_THREAD_IDX < RADIX_BIN)
    {
        for (int k = int(SUBGROUP_THREAD_IDX + RADIX_BIN); k < BIN_HISTS_SIZE; k += RADIX_BIN)
        {
            g_subgroupHists[SUBGROUP_THREAD_IDX] += g_subgroupHists[k];
            g_subgroupHists[k] = g_subgroupHists[SUBGROUP_THREAD_IDX] - g_subgroupHists[k];
        }
        g_reductionHist[SUBGROUP_THREAD_IDX] = g_subgroupHists[SUBGROUP_THREAD_IDX];
        
        // partitionIndex == 0 means it is the first partition, so  the value is the inclusive global prefix sum of the digit counts of this and all previous tiles
        if (partitionIndex == 0){
            atomicAdd(b_passHist[SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex][pass_num], FLAG_INCLUSIVE ^ (g_subgroupHists[SUBGROUP_THREAD_IDX] << 2));
        }
        else{
        // otherwsie,  value is the local, per-tile digit count
            atomicAdd(b_passHist[SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex][pass_num], FLAG_AGGREGATE ^ (g_subgroupHists[SUBGROUP_THREAD_IDX] << 2));
        }

    }
    memoryBarrierBuffer();
    barrier(); // a barrier must be placed between here and the lookback to prevent accidentally broadcasting an incorrect aggregate
    
    //exclusive prefix sum across the reductions
    if (SUBGROUP_THREAD_IDX < RADIX_BIN)
        g_reductionHist[(LANE + 1 & LANE_MASK) + (SUBGROUP_IDX << LANE_LOG)] = subgroupExclusiveAdd(g_reductionHist[SUBGROUP_THREAD_IDX]) + g_reductionHist[SUBGROUP_THREAD_IDX];
    memoryBarrierBuffer();
    barrier();
        
    if (LANE < (RADIX_BIN >> LANE_LOG) && SUBGROUP_IDX == 0)
        g_reductionHist[LANE << LANE_LOG] = subgroupExclusiveAdd(g_reductionHist[LANE << LANE_LOG]);
    memoryBarrierBuffer();
    barrier();

    if (SUBGROUP_THREAD_IDX < RADIX_BIN && LANE != 0)
        g_reductionHist[SUBGROUP_THREAD_IDX] += subgroupBroadcast(g_reductionHist[SUBGROUP_THREAD_IDX - 1], 1);
    memoryBarrierBuffer();
    barrier();
    //
    //Update offsets
    if (SUBGROUP_IDX != 0)
    {
        const uint t = SUBGROUP_IDX << RADIX_LOG;

        if (pass_num == 3){
            for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            {
                const uint t2 = keys[i] >> 24;
                offsets[i] += g_subgroupHists[t2 + t] + g_reductionHist[t2];
            }
        }else{   
        //[unroll]
            for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            {
                const uint t2 = keys[i] >> (RADIX_LOG * pass_num) & RADIX_MASK;
                offsets[i] += g_subgroupHists[t2 + t] + g_reductionHist[t2];
            }
        }
    }
    else
    {
        //[unroll]
        if (pass_num == 3){
        for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            offsets[i] += g_reductionHist[keys[i] >> 24];
        }else{
            for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i)
            offsets[i] += g_reductionHist[keys[i] >> (RADIX_LOG * pass_num) & RADIX_MASK];
        }
    }
    groupMemoryBarrier();
    barrier();
        
    //Scatter keys into shared memory
    // now the g_subgroupHists contain the offset of each scattered key in the subgroup
    
    for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i){
        g_subgroupHists[offsets[i]] = keys[i];
       // debugPrintfEXT("g_subgroupHists[%u]: %u\n", offsets[i], g_subgroupHists[offsets[i]]);
    }
    groupMemoryBarrier();
    barrier();
    /*
    if( gl_GlobalInvocationID.x == 0){
        for (int i = 0; i < BIN_KEYS_PER_THREAD; ++i){
            debugPrintfEXT("offsets[%u]: %u \t keys[%u]: %u\n", i, offsets[i], i, keys[i]);
        }
    }
    */
    // The results are correct up to this point
    //Lookback
    // block 31 scan = block 30 scan result + current block prefix sum
    // block 31 = block 29 scan result + block 30 prefix sum + current block prefix sum
    if (partitionIndex != 0)
    {
        //Free up shared memory, because we are at max
        // set t to be scattered key.
        const uint t = g_subgroupHists[SUBGROUP_IDX];
       // debugPrintfEXT("global invocation id: %u\n sub group id: %u\n sub group invocation id: %u\n workgroup id: %u\n ",  
        //    gl_GlobalInvocationID.x, gl_SubgroupID, gl_SubgroupInvocationID,  gl_WorkGroupID.x);
        // for loop at block level
        //debugPrintfEXT("partitionIndex - LANE - 1: %d\n", int(partitionIndex - LANE - 1));
        for (int i = int(SUBGROUP_IDX); i < RADIX_BIN; i += BIN_SUBGROUPS)
        {
            uint aggregate = 0;

            // set the prefix sum for current subgroup to be the partitionIndex
            if (LANE == 0)
                g_subgroupHists[SUBGROUP_IDX] = partitionIndex;
            // Within the same block, look backward progressively to find the predecessors that record the global inclusive prefix up to and including current tile.
            // todo: delete the lane-count
            //if (partitionIndex -1 >= LANE){
            for (int k = int(partitionIndex - LANE - 1); 0 <= k;)
            {
                
                //debugPrintfEXT("before: %d, after: %d\n", SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex, i * BIN_PARTITIONS + k);
                // get the reduction sum on the predecessors bin in current block
                uint flagPayload = b_passHist[i * BIN_PARTITIONS + k][pass_num];
                // if the reduction sum on the predecessors bin is ready for all threads in current subgroup
                //debugPrintfEXT("i * BIN_PARTITIONS + k = %u, flagPayload & FLAG_MASK  > FLAG_NOT_READY: %d\n", i * BIN_PARTITIONS + k,  (flagPayload & FLAG_MASK) > FLAG_NOT_READY);
                if (subgroupAll((flagPayload & FLAG_MASK) > FLAG_NOT_READY))
                {
                    
                    //debugPrintfEXT("flagPayload & FLAG_MASK  > FLAG_NOT_READY: %d\n", (flagPayload & FLAG_MASK) > FLAG_NOT_READY);
                    
                    
                    // if the predecessors record the global inclusive prefix up to and including current tile
                    if ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE)
                    {   
                        //debugPrintfEXT("okkk\n" );
                        
                        // if current lane has smallest index in the subgroup, then simply set the value to k as index k indicates that  represent the global inclusive prefix sum
                        if (subgroupElect()){
                            g_subgroupHists[SUBGROUP_IDX] = k;
                        }
                        
                            
                    }
                    //debugPrintfEXT("g_subgroupHists[%u]: %u, k: %d\n", SUBGROUP_IDX, g_subgroupHists[SUBGROUP_IDX], k);
                    
                    // if the prefix sum of current subgroup is less than partitionIndex, which means we set g_subgroupHists[SUBGROUP_IDX] to be k in the previous if statement
                    // which means the current subgroup hist record the global inclusive prefix up to and including current tile.
                    if (g_subgroupHists[SUBGROUP_IDX] < partitionIndex)
                    {
                        // sum up the prefix sum of each lane in the subgroup that is ready (which we set to be k in the previous if statement)
                        aggregate += subgroupAdd(k >= g_subgroupHists[SUBGROUP_IDX] ? (flagPayload >> 2) : 0);
                        
                        // if current lane is the first lane in the subgroup
                        if (LANE == 0)
                        {
                            //  then we  add the aggregate sum to the reduction sum of current block and end the lookback for current block
                            atomicAdd(b_passHist[i * BIN_PARTITIONS + partitionIndex][pass_num], 1 ^ (aggregate << 2));
                            // if the subgroup is not the first subgroup, then we add the aggregate sum to the global prefix sum until the previous subgroup
                            g_reductionHist[i] = aggregate + (i != 0 ? g_localHist[i] : 0) - g_reductionHist[i];
                        }
                        break;
                    }
                    // if the prefix sum of current subgroup is greater or equal to partitionIndex,
                    // which means the current subgroup hist contains the aggregate prefix sum of the k-th bin in current block
                    // 
                    else
                    {
                        //debugPrintfEXT("partitionIndex: %u, g_subgroupHists[%u]: %u, k: %d\n", partitionIndex, SUBGROUP_IDX, g_subgroupHists[SUBGROUP_IDX], k);
                        aggregate += subgroupAdd(flagPayload >> 2);
                        k -= LANE_COUNT;
                    }
                    
                }
            }
       // }
            
        }
            
        //place value back
        if (LANE == 0)
            g_subgroupHists[SUBGROUP_IDX] = t;
            
    }
    
    else
    {
        if (SUBGROUP_THREAD_IDX < RADIX_BIN)
            g_reductionHist[SUBGROUP_THREAD_IDX] = (SUBGROUP_THREAD_IDX != 0 ? g_localHist[SUBGROUP_THREAD_IDX] : 0) - g_reductionHist[SUBGROUP_THREAD_IDX];
    }
    groupMemoryBarrier();
    barrier();
    
    //Scatter runs of keys into device memory;
    

    if (pass_num == 3){
        // fuck it is always 0 for g_subgroupHists[i];
        for (uint i = SUBGROUP_THREAD_IDX; i < BIN_PART_SIZE; i += BIN_THREADS)
            b_sort[g_reductionHist[g_subgroupHists[i] >> 24] + i] = g_subgroupHists[i];
    }else{
        for (uint i = SUBGROUP_THREAD_IDX; i < BIN_PART_SIZE; i += BIN_THREADS){
            if (pass_num == 0 || pass_num == 2){
                b_alt[g_reductionHist[g_subgroupHists[i] >> (RADIX_LOG * pass_num) & RADIX_MASK] + i] = 10;//g_subgroupHists[i];
            }else{
                b_sort[g_reductionHist[g_subgroupHists[i] >> (RADIX_LOG * pass_num) & RADIX_MASK] + i] = 10;//g_subgroupHists[i];
            }
        }
    }

    /*
    //for input sizes which are not perfect multiples of the partition tile size
    if (partitionIndex == BIN_PARTITIONS - 1)
    {
        if (SUBGROUP_THREAD_IDX < RADIX_BIN)
            g_reductionHist[SUBGROUP_THREAD_IDX] = (b_passHist[SUBGROUP_THREAD_IDX * BIN_PARTITIONS + partitionIndex][pass_num] >> 2) + (SUBGROUP_THREAD_IDX != 0 ? g_localHist[SUBGROUP_THREAD_IDX] : 0);
        groupMemoryBarrier();
        barrier();
        
        partitionIndex++;
        for (uint i = SUBGROUP_THREAD_IDX + BIN_PART_START; i < input_size; i += BIN_THREADS)
        {
            const uint key = (pass_num %2 == 0 ? b_sort[i] : b_alt[i]);
            uint offset = 0xFFFFFFFF;
            
           // [unroll]
            for (uint k = pass_num * RADIX_LOG; k < ((pass_num + 1) * RADIX_LOG); ++k)
            {
                const bool t = (key >> k & 1) != 0;
                offset &= (t ? 0 : 0xFFFFFFFF) ^ subgroupBallot(t)[0];
            }
            
            //[unroll]
            for (int k = 0; k < BIN_SUBGROUPS; ++k)
            {
                if (SUBGROUP_IDX == k)
                {
                    if(pass_num == 3){
                        const uint t = g_reductionHist[(key >> 24)];
                        if (bitCount(offset << LANE_MASK - LANE) == 1)
                            g_reductionHist[key >> 24] += bitCount(offset);
                        offset = t + bitCount((offset << LANE_MASK - LANE) << 1);
                    }else{
                        const uint t = g_reductionHist[key >> (RADIX_LOG*pass_num) & RADIX_MASK];
                        if (bitCount(offset << LANE_MASK - LANE) == 1)
                            g_reductionHist[key >> (RADIX_LOG*pass_num) & RADIX_MASK] += bitCount(offset);
                        offset = t + bitCount((offset << LANE_MASK - LANE) << 1);
                    }

                }
                groupMemoryBarrier();
                barrier();
            }
            if (pass_num % 2 == 0){
                b_alt[offset] = key;
            }else{
                b_sort[offset] = key;
            }
        }
    }
    */
    
}

void initialization(){
  for(int i = 0; i < 256; i++){
    g_reductionHist[i] = 0;
    b_index[i] = 0;
  }
  for( int i = 0; i < RADIX_BIN; i++){
    g_localHist[i] = 0;
    g_globalHist[i][0] = 0;
    g_globalHist[i][1] = 0;
    g_globalHist[i][2] = 0;
    g_globalHist[i][3] = 0;
  }
  for( int i = 0; i < BIN_PART_SIZE; i++){
    g_subgroupHists[i] = 0;
  }
  for(int i = 0; i < input_size; i++){
    b_alt[i] = 0;
    b_globalHist[i] = 0;
    b_passHist[i] = uvec4(0, 0, 0, 0);

  }
}

void main(){
    if (gl_GlobalInvocationID.x == 0) {
        debugPrintfEXT("subgroups %u, subgroup_threads: %u\n", gl_NumSubgroups, gl_SubgroupSize);

    }
  initialization();
  groupMemoryBarrier();
  barrier();
  GlobalHistogram();
  groupMemoryBarrier();
  barrier();
  BinningPass(0);
  groupMemoryBarrier();
  barrier();
  
  BinningPass(1);
  groupMemoryBarrier();
  barrier();
  
  BinningPass(2);
  groupMemoryBarrier();
  barrier();
  BinningPass(3);
  groupMemoryBarrier();
  barrier();
  
  //barrier();
  //test();
  /*
  barrier();
  BinningPass(1);
  barrier();
  BinningPass(2);
  barrier();
  BinningPass(3);
    */
}